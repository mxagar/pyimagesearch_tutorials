{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92d5055-a01b-45c5-84e2-b56c130868d2",
   "metadata": {},
   "source": [
    "# Contrastive Learning with MNIST Using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cd0bc6a-c6b8-4210-8b7c-c82fb6b6a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8ad657-e74a-4a5e-8bac-fde232b23630",
   "metadata": {},
   "source": [
    "## GPU Setup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a8232dc-efe8-429c-9e7f-664e1a3cbb8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0+cu117'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__\n",
    "# '1.13.0+cu117'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47076de7-b218-4fe6-bcaa-d69c58776cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul  5 16:03:24 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 536.23                 Driver Version: 536.23       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060      WDDM  | 00000000:22:00.0 Off |                  N/A |\n",
      "|  0%   42C    P8              14W / 170W |      0MiB / 12288MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Get info of all GPU devices\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cead982-e8ba-4503-ae99-07eae86ceb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1\n"
     ]
    }
   ],
   "source": [
    "# Set environment variable with possible device ids\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "# Set device: 0 or 1\n",
    "# NOTE: indices are not necessarily the ones shown by nvidia-smi\n",
    "# We need to try them with the cell below\n",
    "torch.cuda.set_device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b938c2d0-bf09-439f-b871-7e457a15a49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version? 1.13.0+cu117\n",
      "Torchvision version? 0.14.0+cu117\n",
      "Is cuda available? True\n",
      "Is cuDNN version: 8500\n",
      "cuDNN enabled?  True\n",
      "Device count? 1\n",
      "Current device? 0\n",
      "Device name?  NVIDIA GeForce RTX 3060\n",
      "tensor([[0.6551, 0.6354, 0.3360],\n",
      "        [0.9716, 0.3431, 0.2295],\n",
      "        [0.0047, 0.9451, 0.6091],\n",
      "        [0.7306, 0.5835, 0.1021],\n",
      "        [0.2828, 0.6482, 0.2419]])\n"
     ]
    }
   ],
   "source": [
    "# Check that the selected device is the desired one\n",
    "print(\"Torch version?\", torch.__version__)\n",
    "print(\"Torchvision version?\", torchvision.__version__)\n",
    "print(\"Is cuda available?\", torch.cuda.is_available())\n",
    "print(\"Is cuDNN version:\", torch.backends.cudnn.version())\n",
    "print(\"cuDNN enabled? \", torch.backends.cudnn.enabled)\n",
    "print(\"Device count?\", torch.cuda.device_count())\n",
    "print(\"Current device?\", torch.cuda.current_device())\n",
    "print(\"Device name? \", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd3d140-42ab-4ff6-af63-0479b60e6797",
   "metadata": {},
   "source": [
    "## Config and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cba87c0-85fb-4826-97a8-ab53b88a3f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = 0.001\n",
    "        self.num_epochs = 10\n",
    "        self.batch_size = 64\n",
    "        self.patience = 5  # For early stopping\n",
    "        self.dropout_p = 0.5\n",
    "        self.embedding_size = 128  # Size of the embedding/feature vectors\n",
    "        self.scheduler_step_size = 10  # Step size for the learning rate scheduler\n",
    "        self.scheduler_gamma = 0.1  # Gamma for the learning rate scheduler: every step_size lr is multiplied by gamma\n",
    "        self.img_shape = (28, 28)  # Not used in this application\n",
    "        # Other application variables\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.base_path = \"./output\"\n",
    "        os.makedirs(self.base_path, exist_ok=True)  # Create the base_path directory if it doesn't exist\n",
    "        self.model_path = os.path.join(self.base_path, \"best_model.pth\")\n",
    "        self.learning_plot_path = os.path.join(self.base_path, \"learning_curves.png\")\n",
    "        self.threshold_plot_path = os.path.join(self.base_path, \"threshold_histogram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52d1d177-646d-4cb7-b99c-3c5ffb9b37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generator class\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, mnist_dataset):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img1, label1 = self.mnist_dataset[index]\n",
    "        index2 = torch.randint(len(self.mnist_dataset), size=(1,)).item()\n",
    "        img2, label2 = self.mnist_dataset[index2]\n",
    "        return img1, img2, torch.tensor(int(label1 == label2), dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b06ace3-d761-46a1-8277-7a6e810b6af2",
   "metadata": {},
   "source": [
    "## Definitions: Siamese Network, Train, Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbb1b841-a16e-4e79-9b13-59aa9d7e5bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese Network\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_size=128, dropout_p=0.3):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        # Remove the fully connected layer\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])\n",
    "\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            #nn.Linear(self.backbone[-1].out_features, 512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout_p),\n",
    "            nn.Linear(256, self.embedding_size)\n",
    "        )\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        return output1, output2\n",
    "\n",
    "# Contrastive loss\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        # pairwise_distance(): equivalent to euclidean_distance:\n",
    "        # torch.sqrt(((output1 - output2) ** 2).sum(dim=1))\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive\n",
    "\n",
    "# Save model function\n",
    "def save_model(model, save_path):\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "# Load model function\n",
    "def load_model(model, load_path, device):\n",
    "    model.load_state_dict(torch.load(load_path, map_location=device))\n",
    "    return model\n",
    "        \n",
    "# Training function\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, scheduler, config):\n",
    "    model.train()\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve_epochs = 0\n",
    "    for epoch in range(config.num_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss = 0\n",
    "        for i, (img1, img2, labels) in enumerate(train_loader):\n",
    "            img1, img2, labels = img1.to(config.device), img2.to(config.device), labels.to(config.device)\n",
    "            output1, output2 = model(img1, img2)\n",
    "            loss = criterion(output1, output2, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Print training loss every 100 batches\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch: {epoch+1}, Batch: {i+1}, Loss: {loss.item()}\")\n",
    "\n",
    "        scheduler.step()\n",
    "        train_loss_history.append(train_loss / len(train_loader))\n",
    "\n",
    "        val_loss = validate(model, val_loader, criterion, config)\n",
    "        val_loss_history.append(val_loss)\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}, Loss: {train_loss_history[-1]}, Val Loss: {val_loss}, Time: {epoch_time}s, Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_model(model, config.model_path)\n",
    "            no_improve_epochs = 0\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            if no_improve_epochs >= config.patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    return train_loss_history, val_loss_history\n",
    "\n",
    "\n",
    "# Validation function\n",
    "def validate(model, val_loader, criterion, config):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (img1, img2, labels) in enumerate(val_loader):\n",
    "            img1, img2, labels = img1.to(config.device), img2.to(config.device), labels.to(config.device)\n",
    "            output1, output2 = model(img1, img2)\n",
    "            loss = criterion(output1, output2, labels)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df483f61-b093-46ef-ae09-a4dbece3922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training function\n",
    "def plot_training(train_loss_history, val_loss_history, config):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_loss_history, label='Train Loss')\n",
    "    plt.plot(val_loss_history, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(config.learning_plot_path)\n",
    "    plt.show()\n",
    "    \n",
    "# Prediction function\n",
    "def predict(model, img1, img2):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output1, output2 = model(img1, img2)\n",
    "        return euclidean_distance(output1, output2)\n",
    "\n",
    "# Plot prediction function\n",
    "def plot_prediction(img1, img2, distances, limit=None):\n",
    "    if limit is not None:\n",
    "        img1, img2, distances = img1[:limit], img2[:limit], distances[:limit]\n",
    "\n",
    "    fig, axs = plt.subplots(len(img1), 2, figsize=(5, len(img1)*5))\n",
    "    for i in range(len(img1)):\n",
    "        axs[i, 0].imshow(img1[i].squeeze(), cmap='gray')\n",
    "        axs[i, 1].imshow(img2[i].squeeze(), cmap='gray')\n",
    "        axs[i, 1].set_title(f\"Distance: {distances[i].item():.2f}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e688d5cb-5b53-4316-910d-5ffad92c92a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate(model, test_loader, config):\n",
    "    model = model.to('cpu')\n",
    "    model.eval()\n",
    "    positive_distances = []\n",
    "    negative_distances = []\n",
    "    labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img1, img2, labels in tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "            output1, output2 = model(img1, img2)\n",
    "            distances = F.pairwise_distance(output1, output2).detach().numpy()\n",
    "            labels = labels.numpy()\n",
    "\n",
    "            positive_distances.extend(distances[labels == 1])\n",
    "            negative_distances.extend(distances[labels == 0])\n",
    "            labels_list.extend(labels)\n",
    "\n",
    "    # Compute best threshold\n",
    "    distances = positive_distances + negative_distances\n",
    "    labels = np.array([1]*len(positive_distances) + [0]*len(negative_distances))\n",
    "    fpr, tpr, thresholds = roc_curve(labels, distances, pos_label=0)\n",
    "    best_threshold = thresholds[np.argmax(tpr - fpr)]\n",
    "\n",
    "    # Compute histograms\n",
    "    plt.hist(positive_distances, bins=30, alpha=0.5, color='r', label='Positive pairs')\n",
    "    plt.hist(negative_distances, bins=30, alpha=0.5, color='b', label='Negative pairs')\n",
    "    \n",
    "    # Plot best threshold\n",
    "    plt.axvline(x=best_threshold, color='g', linestyle='--', label=f'Best threshold: {best_threshold:.2f}')\n",
    "    plt.legend()\n",
    "    plt.savefig(config.threshold_plot_path)\n",
    "    plt.show()\n",
    "\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d880ae97-f56c-4756-9d03-735a94783f74",
   "metadata": {},
   "source": [
    "## Main Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13253483-ad6a-4757-8611-b573bbcd1714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "def main(do_train=True):\n",
    "    config = Config()\n",
    "\n",
    "    # Define the transformations for the training set\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((230, 230)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # Convert grayscale to RGB\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Define the transformations for the validation and test sets\n",
    "    val_test_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # Convert grayscale to RGB\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Load MNIST dataset\n",
    "    mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=train_transform)\n",
    "    print(\"Dataset obtained!\")\n",
    "\n",
    "    # Split the dataset into train, validation, and test sets\n",
    "    train_size = int(0.7 * len(mnist_dataset))\n",
    "    val_size = int(0.15 * len(mnist_dataset))\n",
    "    test_size = len(mnist_dataset) - train_size - val_size\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(mnist_dataset, [train_size, val_size, test_size])\n",
    "    print(\"Dataset splits created!\")\n",
    "\n",
    "    # Apply the appropriate transformations to the validation and test sets\n",
    "    val_dataset.dataset.transform = val_test_transform\n",
    "    test_dataset.dataset.transform = val_test_transform\n",
    "\n",
    "    # Create PairDataset for each split\n",
    "    train_dataset = PairDataset(train_dataset)\n",
    "    val_dataset = PairDataset(val_dataset)\n",
    "    test_dataset = PairDataset(test_dataset)\n",
    "\n",
    "    # Create DataLoader for each split\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "    print(\"Dataset loaders created!\")\n",
    "\n",
    "    # Instantiate the model, criterion, optimizer, and scheduler\n",
    "    model = SiameseNetwork(embedding_size=config.embedding_size,\n",
    "                           dropout_p=config.dropout_p).to(config.device)\n",
    "    criterion = ContrastiveLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=config.learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=config.scheduler_step_size, gamma=config.scheduler_gamma)\n",
    "    print(\"Model instantiated!\")\n",
    "    \n",
    "    # Train the model\n",
    "    if do_train:\n",
    "        print(\"Starting training...\")\n",
    "        train_loss_history, val_loss_history = train(model, train_loader, val_loader, criterion, optimizer, scheduler, config)\n",
    "        print(\"Training completed!\")\n",
    "    \n",
    "        # Plot training history\n",
    "        plot_training(train_loss_history, val_loss_history, config)\n",
    "\n",
    "    # Load the best model\n",
    "    model = SiameseNetwork(embedding_size=config.embedding_size,\n",
    "                           dropout_p=config.dropout_p).to(config.device)\n",
    "    model = load_model(model, config.model_path, config.device)\n",
    "\n",
    "    # Evaluate\n",
    "    print(\"Evaluating model...\")\n",
    "    best_threshold = evaluate(model, test_loader, config)\n",
    "    print(f\"Best threshold: {best_threshold}\")\n",
    "    print(\"Evaluation completed!\")\n",
    "    \n",
    "    # Test the model\n",
    "    test_img1, test_img2, _ = next(iter(test_loader))\n",
    "    test_img1, test_img2 = test_img1.to(config.device), test_img2.to(config.device)\n",
    "    distances = predict(model, test_img1, test_img2)\n",
    "\n",
    "    # Plot predictions\n",
    "    plot_prediction(test_img1.cpu(), test_img2.cpu(), distances.cpu(), limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15d2e25c-c59a-4231-8252-473c34acc1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1f22634-4ea5-4f26-b10e-710d8ae6c351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset obtained!\n",
      "Dataset splits created!\n",
      "Dataset loaders created!\n",
      "Model instantiated!\n",
      "Evaluating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████| 141/141 [19:33<00:00,  8.32s/batch]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold: inf\n",
      "Evaluation completed!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#main(do_train=True)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdo_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 80\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(do_train)\u001b[0m\n\u001b[0;32m     78\u001b[0m test_img1, test_img2, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(test_loader))\n\u001b[0;32m     79\u001b[0m test_img1, test_img2 \u001b[38;5;241m=\u001b[39m test_img1\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mdevice), test_img2\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 80\u001b[0m distances \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_img1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_img2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Plot predictions\u001b[39;00m\n\u001b[0;32m     83\u001b[0m plot_prediction(test_img1\u001b[38;5;241m.\u001b[39mcpu(), test_img2\u001b[38;5;241m.\u001b[39mcpu(), distances\u001b[38;5;241m.\u001b[39mcpu(), limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 16\u001b[0m, in \u001b[0;36mpredict\u001b[1;34m(model, img1, img2)\u001b[0m\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 16\u001b[0m     output1, output2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m euclidean_distance(output1, output2)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\siam\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[15], line 30\u001b[0m, in \u001b[0;36mSiameseNetwork.forward\u001b[1;34m(self, input1, input2)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input1, input2):\n\u001b[1;32m---> 30\u001b[0m     output1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m     output2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_one(input2)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output1, output2\n",
      "Cell \u001b[1;32mIn[15], line 24\u001b[0m, in \u001b[0;36mSiameseNetwork.forward_one\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_one\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 24\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     26\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\siam\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\siam\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\siam\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\siam\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\siam\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "#main(do_train=True)\n",
    "main(do_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cedb31-2e3c-4c32-bb86-abc82a3c4803",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
