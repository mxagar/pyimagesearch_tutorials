{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92d5055-a01b-45c5-84e2-b56c130868d2",
   "metadata": {},
   "source": [
    "# Contrastive Learning with MNIST Using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cd0bc6a-c6b8-4210-8b7c-c82fb6b6a7eb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from tqdm import tqdm\n",
    "import optuna\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8ad657-e74a-4a5e-8bac-fde232b23630",
   "metadata": {},
   "source": [
    "## GPU Setup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a8232dc-efe8-429c-9e7f-664e1a3cbb8d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0+cu117'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__\n",
    "# '1.13.0+cu117'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47076de7-b218-4fe6-bcaa-d69c58776cba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul  7 17:33:03 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 536.23                 Driver Version: 536.23       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060      WDDM  | 00000000:22:00.0 Off |                  N/A |\n",
      "|  0%   47C    P8              14W / 170W |      0MiB / 12288MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Get info of all GPU devices\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cead982-e8ba-4503-ae99-07eae86ceb1f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1\n"
     ]
    }
   ],
   "source": [
    "# Set environment variable with possible device ids\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "# Set device: 0 or 1\n",
    "# NOTE: indices are not necessarily the ones shown by nvidia-smi\n",
    "# We need to try them with the cell below\n",
    "torch.cuda.set_device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b938c2d0-bf09-439f-b871-7e457a15a49e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version? 1.13.0+cu117\n",
      "Torchvision version? 0.14.0+cu117\n",
      "Is cuda available? True\n",
      "Is cuDNN version: 8500\n",
      "cuDNN enabled?  True\n",
      "Device count? 1\n",
      "Current device? 0\n",
      "Device name?  NVIDIA GeForce RTX 3060\n",
      "tensor([[0.8922, 0.5638, 0.0425],\n",
      "        [0.6749, 0.2590, 0.7286],\n",
      "        [0.4643, 0.6446, 0.2752],\n",
      "        [0.3932, 0.7982, 0.8686],\n",
      "        [0.2315, 0.2083, 0.2703]])\n"
     ]
    }
   ],
   "source": [
    "# Check that the selected device is the desired one\n",
    "print(\"Torch version?\", torch.__version__)\n",
    "print(\"Torchvision version?\", torchvision.__version__)\n",
    "print(\"Is cuda available?\", torch.cuda.is_available())\n",
    "print(\"Is cuDNN version:\", torch.backends.cudnn.version())\n",
    "print(\"cuDNN enabled? \", torch.backends.cudnn.enabled)\n",
    "print(\"Device count?\", torch.cuda.device_count())\n",
    "print(\"Current device?\", torch.cuda.current_device())\n",
    "print(\"Device name? \", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd3d140-42ab-4ff6-af63-0479b60e6797",
   "metadata": {},
   "source": [
    "## Config and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cba87c0-85fb-4826-97a8-ab53b88a3f38",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configuration class\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Hyperparameters\n",
    "        # Best hyperparameters: {'lr': 0.00010421719087358276, 'weight_decay': 1.0889499856024069e-08}\n",
    "        self.learning_rate = 0.001 # 0.00010421719087358276\n",
    "        self.num_epochs = 200\n",
    "        self.batch_size = 64\n",
    "        self.patience = 100 # For early stopping\n",
    "        self.dropout_p = 0.3\n",
    "        self.weight_decay = 1e-5 # 1.0889499856024069e-08\n",
    "        self.embedding_size = 32 # Size of the embedding/feature vectors\n",
    "        self.scheduler_step_size = 70  # Step size for the learning rate scheduler\n",
    "        self.scheduler_gamma = 0.3  # Gamma for the learning rate scheduler: every step_size lr is multiplied by gamma\n",
    "        # Other application variables\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #self.device = torch.device(\"cpu\")\n",
    "        self.base_path = \"./output\"\n",
    "        os.makedirs(self.base_path, exist_ok=True)  # Create the base_path directory if it doesn't exist\n",
    "        self.best_model_path = os.path.join(self.base_path, \"best_model.pth\")\n",
    "        self.last_model_path = os.path.join(self.base_path, \"last_model.pth\")\n",
    "        self.learning_plot_path = os.path.join(self.base_path, \"learning_curves.png\")\n",
    "        self.threshold_plot_path = os.path.join(self.base_path, \"threshold_histogram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52d1d177-646d-4cb7-b99c-3c5ffb9b37e0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset generator class\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.pairs = self._make_pairs()\n",
    "\n",
    "    def _make_pairs(self):\n",
    "        # initialize an empty list to hold the image pairs and labels\n",
    "        pairs = []\n",
    "\n",
    "        # calculate the total number of classes present in the dataset\n",
    "        # and then build a list of indexes for each class label that\n",
    "        # provides the indexes for all examples with a given label\n",
    "        # NOTE self.dataset.dataset accesses to the entire dataset\n",
    "        # self.dataset.indices contains the indices of the Subset/current split\n",
    "        #num_classes = len(np.unique(self.dataset.dataset.targets[self.dataset.indices]))\n",
    "        num_classes = len(np.unique(self.dataset.dataset.targets)) # classes in ENTIRE dataset\n",
    "        # Indices per class in SUBSET\n",
    "        indices = [np.where(self.dataset.dataset.targets[self.dataset.indices] == i)[0] for i in range(num_classes)]\n",
    "        # Negative indices per class in SUBSET: indices of images of different class\n",
    "        neg_indices = [np.where(self.dataset.dataset.targets[self.dataset.indices] != i)[0] for i in range(num_classes)]\n",
    "\n",
    "        # loop over all images\n",
    "        for idx_a in range(len(self.dataset)):\n",
    "            # grab the current image and label belonging to the current iteration\n",
    "            _, label = self.dataset[idx_a]\n",
    "\n",
    "            # randomly pick an image that belongs to the *same* class label\n",
    "            idx_b = np.random.choice(indices[label])\n",
    "            pos_image_idx = idx_b\n",
    "\n",
    "            # prepare a positive pair and update the pairs list with the indices and label\n",
    "            pairs.append((idx_a, pos_image_idx, 1))\n",
    "\n",
    "            # grab the indices for each of the class labels *not* equal to the current label\n",
    "            # and randomly pick an image corresponding to a label *not* equal to the current label\n",
    "            #neg_indices = np.where(self.dataset.targets != label)[0]\n",
    "            #neg_indices = np.where(self.dataset.dataset.targets != label)[0]\n",
    "            #neg_image_idx = np.random.choice(neg_indices)\n",
    "            neg_image_idx = np.random.choice(neg_indices[label])\n",
    "\n",
    "            # prepare a negative pair of images and update the pairs list with the indices and label\n",
    "            pairs.append((idx_a, neg_image_idx, 0))\n",
    "\n",
    "        return pairs\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img1_idx, img2_idx, label = self.pairs[index]\n",
    "        img1, label1 = self.dataset[img1_idx]\n",
    "        img2, label2 = self.dataset[img2_idx]\n",
    "        assert (label1 == label2) == bool(label)\n",
    "        return img1, img2, torch.tensor(int(label1 == label2), dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b06ace3-d761-46a1-8277-7a6e810b6af2",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbb1b841-a16e-4e79-9b13-59aa9d7e5bde",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Siamese Network\n",
    "class SiameseNetworkResnet(nn.Module):\n",
    "    def __init__(self, embedding_size=128, dropout_p=0.3, freeze_backbone=False):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        #self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone = models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "        self.embedding_size = embedding_size\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        # Remove the fully connected layer\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            #nn.Linear(self.backbone[-1].out_features, 512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout_p),\n",
    "            nn.Linear(256, self.embedding_size)\n",
    "        )\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        return output1, output2\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_size=48, dropout_p=0.3):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1) # k=2: 28 -> 29; k=3: 28 -> 28\n",
    "        self.pool1 = nn.MaxPool2d(2, 2) # 29/2 -> 14\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc = nn.Linear(64, embedding_size)  # if adaptive_avg_pool2d used below\n",
    "        #self.fc = nn.Linear(64*7*7, embedding_size) # 28/2/2 = 7\n",
    "\n",
    "        # Apply Xavier initialization\n",
    "        nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        nn.init.xavier_uniform_(self.conv2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "    \n",
    "    def forward_one(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        return output1, output2\n",
    "        \n",
    "# Contrastive loss\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        # pairwise_distance(): equivalent to euclidean_distance:\n",
    "        # torch.sqrt(((output1 - output2) ** 2).sum(dim=1))\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f3aa0b-ad78-405f-b961-741520fcb113",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d2a55b1-9224-4e64-82f6-bc126a56d50e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save model function\n",
    "def save_model(model, save_path):\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "# Load model function\n",
    "def load_model(model, load_path, device):\n",
    "    model.load_state_dict(torch.load(load_path, map_location=device))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df483f61-b093-46ef-ae09-a4dbece3922d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot training function\n",
    "def plot_training(train_loss_history, val_loss_history, config):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_loss_history, label='Train Loss')\n",
    "    plt.plot(val_loss_history, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(config.learning_plot_path)\n",
    "    plt.show()\n",
    "    \n",
    "# Prediction function\n",
    "def predict(model, img1, img2, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    img1, img2 = img1.to(device), img2.to(device)\n",
    "    with torch.no_grad():\n",
    "        output1, output2 = model(img1, img2)\n",
    "        distances = F.pairwise_distance(output1, output2).to(\"cpu\").numpy()\n",
    "        return distances\n",
    "\n",
    "# Plot prediction function\n",
    "def plot_prediction(img1, img2, distances, limit=None):\n",
    "    if limit is not None:\n",
    "        img1, img2, distances = img1[:limit], img2[:limit], distances[:limit]\n",
    "\n",
    "    fig, axs = plt.subplots(len(img1), 2, figsize=(5, 3*len(img1)))\n",
    "    for i in range(len(img1)):\n",
    "        img1_i = img1[i].squeeze().permute(1, 2, 0) if img1[i].shape[0] == 3 else img1[i].squeeze()\n",
    "        img2_i = img2[i].squeeze().permute(1, 2, 0) if img2[i].shape[0] == 3 else img2[i].squeeze()\n",
    "        cmap1 = 'gray' if img1[i].shape[0] == 1 else None\n",
    "        cmap2 = 'gray' if img2[i].shape[0] == 1 else None\n",
    "        axs[i, 0].imshow(img1_i, cmap=cmap1)\n",
    "        axs[i, 1].imshow(img2_i, cmap=cmap2)\n",
    "        axs[i, 1].set_title(f\"Distance: {distances[i].item():.2f}\")\n",
    "        #axs[i, 0].imshow(img1[i].squeeze()[0], cmap='gray')  # Select the first channel\n",
    "        #axs[i, 1].imshow(img2[i].squeeze()[0], cmap='gray')  # Select the first channel\n",
    "        axs[i, 1].set_title(f\"Distance: {distances[i].item():.2f}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e3cc2f-9875-404d-b680-2ec5ce96cee3",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a49684c-b443-4002-b39d-c03ef995ac90",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, scheduler, config, output_freq=2, debug=False, limit=None):\n",
    "    model.train()\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve_epochs = 0\n",
    "    total_batches = len(train_loader)\n",
    "    print_every = total_batches // output_freq  # Print every 1/output_freq of total batches\n",
    "    weights_before = None\n",
    "    weights_after = None\n",
    "    limit_val = None\n",
    "    if limit is not None:\n",
    "        # Take same batches ratio for validation\n",
    "        limit_val = max(int(float(len(val_loader)) * float(limit / len(train_loader))), 1)\n",
    "        #print(\"limit_val\", limit_val)\n",
    "        #print(\"limit_train\", limit)\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        if debug:\n",
    "            # Select the weights of a layer\n",
    "            weights_before = model.conv1.weight.detach().clone()\n",
    "\n",
    "        batches = 0\n",
    "        for i, (img1, img2, labels) in enumerate(train_loader):\n",
    "            batches += 1\n",
    "            img1, img2, labels = img1.to(config.device), img2.to(config.device), labels.to(config.device)\n",
    "            output1, output2 = model(img1, img2)\n",
    "            loss = criterion(output1, output2, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Print training loss\n",
    "            if i % print_every == 0:\n",
    "                print(f\"Epoch: {epoch+1}, Batch: {i+1}, Loss: {loss.item()}\")\n",
    "\n",
    "            # Break if the number of batches is above the limit\n",
    "            if limit is not None:\n",
    "                if batches > limit:\n",
    "                    break\n",
    "        \n",
    "        scheduler.step()\n",
    "        #train_loss_history.append(train_loss / len(train_loader))\n",
    "        train_loss_history.append(train_loss / batches)\n",
    "\n",
    "        val_loss = validate(model, val_loader, criterion, config, limit=limit_val)\n",
    "        #val_loss = train_loss / batches\n",
    "        val_loss_history.append(val_loss)\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "\n",
    "        if debug:\n",
    "            weights_after = model.conv1.weight.detach().clone()\n",
    "            print(\"[DEBUG] weights_before (m, s): \", weights_before.mean().item(), weights_before.std().item())\n",
    "            print(\"[DEBUG] weights_after (m, s): \", weights_after.mean().item(), weights_after.std().item())\n",
    "            print(\"[DEBUG] weights changed? \", not torch.equal(weights_before, weights_after))\n",
    "        \n",
    "        print(f\"Epoch: {epoch+1}, Loss: {train_loss_history[-1]}, Val Loss: {val_loss}, Time: {epoch_time}s, Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
    "\n",
    "        # Save last model\n",
    "        save_model(model, config.last_model_path)    \n",
    "\n",
    "        # Save best model & early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_model(model, config.best_model_path)\n",
    "            no_improve_epochs = 0\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            if no_improve_epochs >= config.patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    return train_loss_history, val_loss_history\n",
    "\n",
    "# Validation function\n",
    "def validate(model, val_loader, criterion, config, limit=None):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    batches_val = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (img1, img2, labels) in enumerate(val_loader):\n",
    "            batches_val += 1\n",
    "            img1, img2, labels = img1.to(config.device), img2.to(config.device), labels.to(config.device)\n",
    "            output1, output2 = model(img1, img2)\n",
    "            loss = criterion(output1, output2, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if limit is not None:\n",
    "                if batches_val > limit:\n",
    "                    break\n",
    "                    \n",
    "    #return total_loss / len(val_loader)\n",
    "    return total_loss / batches_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79368783-d79f-44c6-8d52-62bac74026c2",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e688d5cb-5b53-4316-910d-5ffad92c92a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate(model, test_loader, config, limit=None):\n",
    "    model = model.to('cpu')\n",
    "    model.eval()\n",
    "    positive_distances = []\n",
    "    negative_distances = []\n",
    "    labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        count = 0\n",
    "        for img1, img2, labels in tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "            count += 1\n",
    "            output1, output2 = model(img1, img2)\n",
    "            distances = F.pairwise_distance(output1, output2).detach().numpy()\n",
    "            labels = labels.numpy()\n",
    "\n",
    "            positive_distances.extend(distances[labels == 1])\n",
    "            negative_distances.extend(distances[labels == 0])\n",
    "            labels_list.extend(labels)\n",
    "\n",
    "            if limit is not None:\n",
    "                if count > limit:\n",
    "                    break\n",
    "\n",
    "    # Compute best threshold\n",
    "    distances = positive_distances + negative_distances\n",
    "    labels = np.array([1]*len(positive_distances) + [0]*len(negative_distances))\n",
    "    fpr, tpr, thresholds = roc_curve(labels, distances, pos_label=0)\n",
    "    best_threshold = thresholds[np.argmax(tpr - fpr)]\n",
    "\n",
    "    # Compute histograms\n",
    "    plt.hist(negative_distances, bins=30, alpha=0.5, color='r', label='Negative pairs')\n",
    "    plt.hist(positive_distances, bins=30, alpha=0.5, color='b', label='Positive pairs')\n",
    "    \n",
    "    # Plot best threshold\n",
    "    plt.axvline(x=best_threshold, color='g', linestyle='--', label=f'Best threshold: {best_threshold:.2f}')\n",
    "    plt.legend()\n",
    "    plt.savefig(config.threshold_plot_path)\n",
    "    plt.show()\n",
    "\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d880ae97-f56c-4756-9d03-735a94783f74",
   "metadata": {},
   "source": [
    "## Main Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13253483-ad6a-4757-8611-b573bbcd1714",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Main function\n",
    "def main(do_train=\"single\"):\n",
    "    config = Config()\n",
    "    \n",
    "    # Choose the model and the data transformations\n",
    "    resnet = False\n",
    "\n",
    "    # Define the transformations for the training set\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((30, 30)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(28),\n",
    "    ])\n",
    "    \n",
    "    # Define the transformations for the validation and test sets\n",
    "    val_test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    if resnet:\n",
    "        # Define the transformations for the training set\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((230, 230)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # Convert grayscale to RGB\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(224),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # Define the transformations for the validation and test sets\n",
    "        val_test_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # Convert grayscale to RGB\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    # Load MNIST dataset\n",
    "    mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=train_transform)\n",
    "    print(\"Dataset obtained!\")\n",
    "\n",
    "    # Split the dataset into train, validation, and test sets\n",
    "    train_size = int(0.75 * len(mnist_dataset))\n",
    "    val_size = int(0.1 * len(mnist_dataset))\n",
    "    test_size = len(mnist_dataset) - train_size - val_size\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(mnist_dataset, [train_size, val_size, test_size])\n",
    "    print(\"Dataset splits created!\")\n",
    "\n",
    "    # Apply the appropriate transformations to the validation and test sets\n",
    "    val_dataset.dataset.transform = val_test_transform\n",
    "    test_dataset.dataset.transform = val_test_transform\n",
    "\n",
    "    # Create PairDataset for each split\n",
    "    train_dataset = PairDataset(train_dataset)\n",
    "    val_dataset = PairDataset(val_dataset)\n",
    "    test_dataset = PairDataset(test_dataset)\n",
    "\n",
    "    # Create DataLoader for each split\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "    print(\"Dataset loaders created!\")\n",
    "\n",
    "    # Instantiate the model, criterion, optimizer, and scheduler\n",
    "    # We need to have a model also for the case in whic we don't train\n",
    "    model = SiameseNetwork(embedding_size=config.embedding_size,\n",
    "                           dropout_p=config.dropout_p).to(config.device)\n",
    "    if resnet:\n",
    "        model = SiameseNetworkResnet(embedding_size=config.embedding_size,\n",
    "                                     dropout_p=config.dropout_p).to(config.device)    \n",
    "    print(\"Model instantiated!\")\n",
    "    \n",
    "    # Train the model: One Training with hyperparameters from Config\n",
    "    if do_train == \"single\":\n",
    "        # Instantiate the criterion, optimizer, and scheduler\n",
    "        criterion = ContrastiveLoss()\n",
    "        optimizer = Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "        scheduler = StepLR(optimizer, step_size=config.scheduler_step_size, gamma=config.scheduler_gamma)\n",
    "\n",
    "        # Train\n",
    "        print(\"Starting training...\")\n",
    "        train_loss_history, val_loss_history = train(model, train_loader, val_loader, criterion, optimizer, scheduler, config, debug=True)\n",
    "        print(\"Training completed!\")\n",
    "    \n",
    "        # Plot training history\n",
    "        plot_training(train_loss_history, val_loss_history, config)\n",
    "\n",
    "    # Train the model: Hyperparameter Tuning / Search\n",
    "    elif do_train == \"tune\":\n",
    "        print(\"Starting hyperparameter search...\")\n",
    "\n",
    "        def objective(trial):\n",
    "            # Suggest values for the hyperparameters\n",
    "            lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True) # suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "            weight_decay = trial.suggest_float('weight_decay', 1e-10, 1e-3, log=True) # suggest_loguniform('weight_decay', 1e-10, 1e-3)\n",
    "            \n",
    "            # Re-define a CNN model using the hyperparameters suggested by the trial\n",
    "            # Each trial should have a new model, also because we might tune it!\n",
    "            model = SiameseNetwork(embedding_size=config.embedding_size,\n",
    "                                   dropout_p=config.dropout_p).to(config.device)\n",
    "            if resnet:\n",
    "                model = SiameseNetworkResnet(embedding_size=config.embedding_size,\n",
    "                                             dropout_p=config.dropout_p).to(config.device)    \n",
    "\n",
    "            # Define loss, scheduler, optimizer\n",
    "            criterion = ContrastiveLoss()\n",
    "            optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "            scheduler = StepLR(optimizer, step_size=config.scheduler_step_size, gamma=config.scheduler_gamma)\n",
    "\n",
    "            # Run training\n",
    "            old_epochs = config.num_epochs\n",
    "            config.num_epochs = 20\n",
    "            train_loss_history, val_loss_history = train(model, train_loader, val_loader, criterion, optimizer, scheduler, config,\n",
    "                                                         debug=False, limit=len(train_loader)//3) # Only the first N batches used\n",
    "            config.num_epochs = old_epochs\n",
    "            \n",
    "            # Return the final validation loss\n",
    "            #return val_loss_history[-1]\n",
    "            return min(val_loss_history)\n",
    "\n",
    "        # Run experiments\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(objective, n_trials=30)\n",
    "        \n",
    "        best_params = study.best_params\n",
    "        print(f\"Best hyperparameters: {best_params}\")        \n",
    "        print(\"Hyperparameter search completed!\")\n",
    "        \n",
    "    # Load the best model\n",
    "    model = SiameseNetwork(embedding_size=config.embedding_size,\n",
    "                           dropout_p=config.dropout_p).to(config.device)\n",
    "    if resnet:\n",
    "        model = SiameseNetworkResnet(embedding_size=config.embedding_size,\n",
    "                                     dropout_p=config.dropout_p).to(config.device)    \n",
    "    model = load_model(model, config.best_model_path, config.device)\n",
    "\n",
    "    # Evaluate\n",
    "    print(\"Evaluating model...\")\n",
    "    best_threshold = evaluate(model, test_loader, config, limit=None)\n",
    "    print(f\"Best threshold: {best_threshold}\")\n",
    "    print(\"Evaluation completed!\")\n",
    "    \n",
    "    # Test the model\n",
    "    print(\"Predicting random batch...\")\n",
    "    test_img1, test_img2, _ = next(iter(test_loader))\n",
    "    distances = predict(model, test_img1, test_img2)\n",
    "\n",
    "    # Plot predictions\n",
    "    plot_prediction(test_img1.to(\"cpu\"), test_img2.to(\"cpu\"), distances, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15d2e25c-c59a-4231-8252-473c34acc1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f22634-4ea5-4f26-b10e-710d8ae6c351",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset obtained!\n",
      "Dataset splits created!\n",
      "Dataset loaders created!\n",
      "Model instantiated!\n",
      "Starting training...\n",
      "Epoch: 1, Batch: 1, Loss: 0.361662358045578\n",
      "Epoch: 1, Batch: 704, Loss: 0.26281100511550903\n",
      "Epoch: 1, Batch: 1407, Loss: 0.26922595500946045\n",
      "[DEBUG] weights_before (m, s):  -0.001818284043110907 0.05926886573433876\n",
      "[DEBUG] weights_after (m, s):  -0.00969791878014803 0.04855143651366234\n",
      "[DEBUG] weights changed?  True\n",
      "Epoch: 1, Loss: 0.2574013722497856, Val Loss: 0.4579685019685867, Time: 63.53524827957153s, Learning Rate: 0.001\n",
      "Epoch: 2, Batch: 1, Loss: 0.27116161584854126\n",
      "Epoch: 2, Batch: 704, Loss: 0.2856485843658447\n"
     ]
    }
   ],
   "source": [
    "main(do_train=\"single\")\n",
    "#main(do_train=\"no\")\n",
    "#main(do_train=\"tune\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cedb31-2e3c-4c32-bb86-abc82a3c4803",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f014f79-667c-43b4-9cb9-85c044a002ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
