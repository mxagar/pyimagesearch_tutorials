{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92d5055-a01b-45c5-84e2-b56c130868d2",
   "metadata": {},
   "source": [
    "# Contrastive Learning with MNIST Using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cd0bc6a-c6b8-4210-8b7c-c82fb6b6a7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8ad657-e74a-4a5e-8bac-fde232b23630",
   "metadata": {},
   "source": [
    "## GPU Setup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a8232dc-efe8-429c-9e7f-664e1a3cbb8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0+cu117'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__\n",
    "# '1.13.0+cu117'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47076de7-b218-4fe6-bcaa-d69c58776cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul  6 16:13:35 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 536.23                 Driver Version: 536.23       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060      WDDM  | 00000000:22:00.0 Off |                  N/A |\n",
      "|  0%   40C    P8              14W / 170W |      0MiB / 12288MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Get info of all GPU devices\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cead982-e8ba-4503-ae99-07eae86ceb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1\n"
     ]
    }
   ],
   "source": [
    "# Set environment variable with possible device ids\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "# Set device: 0 or 1\n",
    "# NOTE: indices are not necessarily the ones shown by nvidia-smi\n",
    "# We need to try them with the cell below\n",
    "torch.cuda.set_device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b938c2d0-bf09-439f-b871-7e457a15a49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version? 1.13.0+cu117\n",
      "Torchvision version? 0.14.0+cu117\n",
      "Is cuda available? True\n",
      "Is cuDNN version: 8500\n",
      "cuDNN enabled?  True\n",
      "Device count? 1\n",
      "Current device? 0\n",
      "Device name?  NVIDIA GeForce RTX 3060\n",
      "tensor([[0.9684, 0.1714, 0.1893],\n",
      "        [0.9239, 0.8133, 0.0158],\n",
      "        [0.5785, 0.6900, 0.7949],\n",
      "        [0.8359, 0.5665, 0.9882],\n",
      "        [0.5788, 0.4752, 0.3651]])\n"
     ]
    }
   ],
   "source": [
    "# Check that the selected device is the desired one\n",
    "print(\"Torch version?\", torch.__version__)\n",
    "print(\"Torchvision version?\", torchvision.__version__)\n",
    "print(\"Is cuda available?\", torch.cuda.is_available())\n",
    "print(\"Is cuDNN version:\", torch.backends.cudnn.version())\n",
    "print(\"cuDNN enabled? \", torch.backends.cudnn.enabled)\n",
    "print(\"Device count?\", torch.cuda.device_count())\n",
    "print(\"Current device?\", torch.cuda.current_device())\n",
    "print(\"Device name? \", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd3d140-42ab-4ff6-af63-0479b60e6797",
   "metadata": {},
   "source": [
    "## Config and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cba87c0-85fb-4826-97a8-ab53b88a3f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration class\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Hyperparameters\n",
    "        self.learning_rate = 0.001\n",
    "        self.num_epochs = 100\n",
    "        self.batch_size = 64\n",
    "        self.patience = 5 # For early stopping\n",
    "        self.dropout_p = 0.3\n",
    "        self.embedding_size = 48 # Size of the embedding/feature vectors\n",
    "        self.scheduler_step_size = 30  # Step size for the learning rate scheduler\n",
    "        self.scheduler_gamma = 0.1  # Gamma for the learning rate scheduler: every step_size lr is multiplied by gamma\n",
    "        self.img_shape = (28, 28)  # Not used in this application\n",
    "        # Other application variables\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.base_path = \"./output\"\n",
    "        os.makedirs(self.base_path, exist_ok=True)  # Create the base_path directory if it doesn't exist\n",
    "        self.best_model_path = os.path.join(self.base_path, \"best_model.pth\")\n",
    "        self.last_model_path = os.path.join(self.base_path, \"last_model.pth\")\n",
    "        self.learning_plot_path = os.path.join(self.base_path, \"learning_curves.png\")\n",
    "        self.threshold_plot_path = os.path.join(self.base_path, \"threshold_histogram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52d1d177-646d-4cb7-b99c-3c5ffb9b37e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset generator class\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, mnist_dataset):\n",
    "        self.mnist_dataset = mnist_dataset\n",
    "        self.transform = transforms.ToTensor()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img1, label1 = self.mnist_dataset[index]\n",
    "        index2 = torch.randint(len(self.mnist_dataset), size=(1,)).item()\n",
    "        img2, label2 = self.mnist_dataset[index2]\n",
    "        return img1, img2, torch.tensor(int(label1 == label2), dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mnist_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b06ace3-d761-46a1-8277-7a6e810b6af2",
   "metadata": {},
   "source": [
    "## Definitions: Siamese Network, Train, Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbb1b841-a16e-4e79-9b13-59aa9d7e5bde",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Siamese Network\n",
    "class SiameseNetworkResnet(nn.Module):\n",
    "    def __init__(self, embedding_size=128, dropout_p=0.3, freeze_backbone=False):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        #self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone = models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "        self.embedding_size = embedding_size\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        # Remove the fully connected layer\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-1])\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            #nn.Linear(self.backbone[-1].out_features, 512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout_p),\n",
    "            nn.Linear(256, self.embedding_size)\n",
    "        )\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        return output1, output2\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, embedding_size=48, dropout_p=0.3):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)  # Changed input channels to 1 for grayscale images\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        #self.fc = nn.Linear(64, embedding_size)  # if adaptive_avg_pool2d used below\n",
    "        self.fc = nn.Linear(64*7*7, embedding_size) # 28/2/2 = 7\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        #x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        return output1, output2\n",
    "        \n",
    "# Contrastive loss\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        # pairwise_distance(): equivalent to euclidean_distance:\n",
    "        # torch.sqrt(((output1 - output2) ** 2).sum(dim=1))\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive\n",
    "\n",
    "# Save model function\n",
    "def save_model(model, save_path):\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "# Load model function\n",
    "def load_model(model, load_path, device):\n",
    "    model.load_state_dict(torch.load(load_path, map_location=device))\n",
    "    return model\n",
    "        \n",
    "# Training function\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, scheduler, config, output_freq=2):\n",
    "    model.train()\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve_epochs = 0\n",
    "    total_batches = len(train_loader)\n",
    "    print_every = total_batches // output_freq  # Print every 1/output_freq of total batches\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss = 0\n",
    "        model.train()\n",
    "        for i, (img1, img2, labels) in enumerate(train_loader):\n",
    "            img1, img2, labels = img1.to(config.device), img2.to(config.device), labels.to(config.device)\n",
    "            output1, output2 = model(img1, img2)\n",
    "            loss = criterion(output1, output2, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Print training loss\n",
    "            if i % print_every == 0:\n",
    "                print(f\"Epoch: {epoch+1}, Batch: {i+1}, Loss: {loss.item()}\")\n",
    "\n",
    "        scheduler.step()\n",
    "        train_loss_history.append(train_loss / len(train_loader))\n",
    "\n",
    "        val_loss = validate(model, val_loader, criterion, config)\n",
    "        val_loss_history.append(val_loss)\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}, Loss: {train_loss_history[-1]}, Val Loss: {val_loss}, Time: {epoch_time}s, Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
    "\n",
    "        # Save last model\n",
    "        save_model(model, config.last_model_path)    \n",
    "\n",
    "        # Save best model & early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_model(model, config.best_model_path)\n",
    "            no_improve_epochs = 0\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            if no_improve_epochs >= config.patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    return train_loss_history, val_loss_history\n",
    "\n",
    "\n",
    "# Validation function\n",
    "def validate(model, val_loader, criterion, config):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (img1, img2, labels) in enumerate(val_loader):\n",
    "            img1, img2, labels = img1.to(config.device), img2.to(config.device), labels.to(config.device)\n",
    "            output1, output2 = model(img1, img2)\n",
    "            loss = criterion(output1, output2, labels)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df483f61-b093-46ef-ae09-a4dbece3922d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training function\n",
    "def plot_training(train_loss_history, val_loss_history, config):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_loss_history, label='Train Loss')\n",
    "    plt.plot(val_loss_history, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(config.learning_plot_path)\n",
    "    plt.show()\n",
    "    \n",
    "# Prediction function\n",
    "def predict(model, img1, img2, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    img1, img2 = img1.to(device), img2.to(device)\n",
    "    with torch.no_grad():\n",
    "        output1, output2 = model(img1, img2)\n",
    "        distances = F.pairwise_distance(output1, output2).to(\"cpu\").numpy()\n",
    "        return distances\n",
    "\n",
    "# Plot prediction function\n",
    "def plot_prediction(img1, img2, distances, limit=None):\n",
    "    if limit is not None:\n",
    "        img1, img2, distances = img1[:limit], img2[:limit], distances[:limit]\n",
    "\n",
    "    fig, axs = plt.subplots(len(img1), 2, figsize=(5, 3*len(img1)))\n",
    "    for i in range(len(img1)):\n",
    "        img1_i = img1[i].squeeze().permute(1, 2, 0) if img1[i].shape[0] == 3 else img1[i].squeeze()\n",
    "        img2_i = img2[i].squeeze().permute(1, 2, 0) if img2[i].shape[0] == 3 else img2[i].squeeze()\n",
    "        cmap1 = 'gray' if img1[i].shape[0] == 1 else None\n",
    "        cmap2 = 'gray' if img2[i].shape[0] == 1 else None\n",
    "        axs[i, 0].imshow(img1_i, cmap=cmap1)\n",
    "        axs[i, 1].imshow(img2_i, cmap=cmap2)\n",
    "        axs[i, 1].set_title(f\"Distance: {distances[i].item():.2f}\")\n",
    "        #axs[i, 0].imshow(img1[i].squeeze()[0], cmap='gray')  # Select the first channel\n",
    "        #axs[i, 1].imshow(img2[i].squeeze()[0], cmap='gray')  # Select the first channel\n",
    "        axs[i, 1].set_title(f\"Distance: {distances[i].item():.2f}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e688d5cb-5b53-4316-910d-5ffad92c92a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate(model, test_loader, config, limit=None):\n",
    "    model = model.to('cpu')\n",
    "    model.eval()\n",
    "    positive_distances = []\n",
    "    negative_distances = []\n",
    "    labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        count = 0\n",
    "        for img1, img2, labels in tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "            count += 1\n",
    "            output1, output2 = model(img1, img2)\n",
    "            distances = F.pairwise_distance(output1, output2).detach().numpy()\n",
    "            labels = labels.numpy()\n",
    "\n",
    "            positive_distances.extend(distances[labels == 1])\n",
    "            negative_distances.extend(distances[labels == 0])\n",
    "            labels_list.extend(labels)\n",
    "\n",
    "            if limit is not None:\n",
    "                if count > limit:\n",
    "                    break\n",
    "\n",
    "    # Compute best threshold\n",
    "    distances = positive_distances + negative_distances\n",
    "    labels = np.array([1]*len(positive_distances) + [0]*len(negative_distances))\n",
    "    fpr, tpr, thresholds = roc_curve(labels, distances, pos_label=0)\n",
    "    best_threshold = thresholds[np.argmax(tpr - fpr)]\n",
    "\n",
    "    # Compute histograms\n",
    "    plt.hist(positive_distances, bins=30, alpha=0.5, color='r', label='Positive pairs')\n",
    "    plt.hist(negative_distances, bins=30, alpha=0.5, color='b', label='Negative pairs')\n",
    "    \n",
    "    # Plot best threshold\n",
    "    plt.axvline(x=best_threshold, color='g', linestyle='--', label=f'Best threshold: {best_threshold:.2f}')\n",
    "    plt.legend()\n",
    "    plt.savefig(config.threshold_plot_path)\n",
    "    plt.show()\n",
    "\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d880ae97-f56c-4756-9d03-735a94783f74",
   "metadata": {},
   "source": [
    "## Main Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13253483-ad6a-4757-8611-b573bbcd1714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function\n",
    "def main(do_train=True):\n",
    "    config = Config()\n",
    "\n",
    "    # Choose the model and the data transformations\n",
    "    resnet = False\n",
    "\n",
    "    # Define the transformations for the training set\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((30, 30)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(28),\n",
    "    ])\n",
    "    \n",
    "    # Define the transformations for the validation and test sets\n",
    "    val_test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    if resnet:\n",
    "        # Define the transformations for the training set\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize((230, 230)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # Convert grayscale to RGB\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(224),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # Define the transformations for the validation and test sets\n",
    "        val_test_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # Convert grayscale to RGB\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    # Load MNIST dataset\n",
    "    mnist_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=train_transform)\n",
    "    print(\"Dataset obtained!\")\n",
    "\n",
    "    # Split the dataset into train, validation, and test sets\n",
    "    train_size = int(0.7 * len(mnist_dataset))\n",
    "    val_size = int(0.15 * len(mnist_dataset))\n",
    "    test_size = len(mnist_dataset) - train_size - val_size\n",
    "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(mnist_dataset, [train_size, val_size, test_size])\n",
    "    print(\"Dataset splits created!\")\n",
    "\n",
    "    # Apply the appropriate transformations to the validation and test sets\n",
    "    val_dataset.dataset.transform = val_test_transform\n",
    "    test_dataset.dataset.transform = val_test_transform\n",
    "\n",
    "    # Create PairDataset for each split\n",
    "    train_dataset = PairDataset(train_dataset)\n",
    "    val_dataset = PairDataset(val_dataset)\n",
    "    test_dataset = PairDataset(test_dataset)\n",
    "\n",
    "    # Create DataLoader for each split\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)\n",
    "    print(\"Dataset loaders created!\")\n",
    "\n",
    "    # Instantiate the model, criterion, optimizer, and scheduler\n",
    "    model = SiameseNetwork(embedding_size=config.embedding_size,\n",
    "                           dropout_p=config.dropout_p).to(config.device)\n",
    "    if resnet:\n",
    "        model = SiameseNetworkResnet(embedding_size=config.embedding_size,\n",
    "                                     dropout_p=config.dropout_p).to(config.device)    \n",
    "    criterion = ContrastiveLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=config.learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=config.scheduler_step_size, gamma=config.scheduler_gamma)\n",
    "    print(\"Model instantiated!\")\n",
    "    \n",
    "    # Train the model\n",
    "    if do_train:\n",
    "        print(\"Starting training...\")\n",
    "        train_loss_history, val_loss_history = train(model, train_loader, val_loader, criterion, optimizer, scheduler, config)\n",
    "        print(\"Training completed!\")\n",
    "    \n",
    "        # Plot training history\n",
    "        plot_training(train_loss_history, val_loss_history, config)\n",
    "\n",
    "    # Load the best model\n",
    "    model = SiameseNetwork(embedding_size=config.embedding_size,\n",
    "                           dropout_p=config.dropout_p).to(config.device)\n",
    "    if resnet:\n",
    "        model = SiameseNetworkResnet(embedding_size=config.embedding_size,\n",
    "                                     dropout_p=config.dropout_p).to(config.device)    \n",
    "    model = load_model(model, config.best_model_path, config.device)\n",
    "\n",
    "    # Evaluate\n",
    "    print(\"Evaluating model...\")\n",
    "    best_threshold = evaluate(model, test_loader, config, limit=None)\n",
    "    print(f\"Best threshold: {best_threshold}\")\n",
    "    print(\"Evaluation completed!\")\n",
    "    \n",
    "    # Test the model\n",
    "    print(\"Predicting random batch...\")\n",
    "    test_img1, test_img2, _ = next(iter(test_loader))\n",
    "    distances = predict(model, test_img1, test_img2)\n",
    "\n",
    "    # Plot predictions\n",
    "    plot_prediction(test_img1.to(\"cpu\"), test_img2.to(\"cpu\"), distances, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15d2e25c-c59a-4231-8252-473c34acc1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "#    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f22634-4ea5-4f26-b10e-710d8ae6c351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset obtained!\n",
      "Dataset splits created!\n",
      "Dataset loaders created!\n",
      "Model instantiated!\n",
      "Starting training...\n",
      "Epoch: 1, Batch: 1, Loss: 0.4373529553413391\n",
      "Epoch: 1, Batch: 329, Loss: 0.07190398871898651\n",
      "Epoch: 1, Batch: 657, Loss: 0.05930083245038986\n",
      "Epoch: 1, Loss: 0.09279891693060212, Val Loss: 0.09919741970199203, Time: 35.2789409160614s, Learning Rate: 0.001\n",
      "Epoch: 2, Batch: 1, Loss: 0.08687381446361542\n",
      "Epoch: 2, Batch: 329, Loss: 0.12331025302410126\n",
      "Epoch: 2, Batch: 657, Loss: 0.1026899442076683\n",
      "Epoch: 2, Loss: 0.09080783464581546, Val Loss: 0.09652836104286901, Time: 31.365543127059937s, Learning Rate: 0.001\n",
      "Epoch: 3, Batch: 1, Loss: 0.12313893437385559\n",
      "Epoch: 3, Batch: 329, Loss: 0.12194577604532242\n"
     ]
    }
   ],
   "source": [
    "main(do_train=True)\n",
    "#main(do_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cedb31-2e3c-4c32-bb86-abc82a3c4803",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f014f79-667c-43b4-9cb9-85c044a002ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
