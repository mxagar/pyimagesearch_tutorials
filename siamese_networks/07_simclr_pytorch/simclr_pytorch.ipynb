{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "784b8183-b160-4df2-b7c1-1cb48523213c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# SimCLR Using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb103a99-deb7-413e-9fd4-4179ec1cb2c7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d16517c-0694-4b70-80d8-2d0e1086374f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## GPU Setup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5b09b96-c24b-46a3-ac23-1b0836fbb12e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.0+cu117'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__\n",
    "# '1.13.0+cu117'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "109422ab-25dc-4ddb-bfba-788bf3c8e6a0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul  7 13:54:39 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 536.23                 Driver Version: 536.23       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060      WDDM  | 00000000:22:00.0 Off |                  N/A |\n",
      "|  0%   49C    P3              36W / 170W |    269MiB / 12288MiB |     37%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     14948      C   ...ocal\\anaconda3\\envs\\siam\\python.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Get info of all GPU devices\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3761cec8-0797-43d8-a06a-bd140efa4bc7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1\n"
     ]
    }
   ],
   "source": [
    "# Set environment variable with possible device ids\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "# Set device: 0 or 1\n",
    "# NOTE: indices are not necessarily the ones shown by nvidia-smi\n",
    "# We need to try them with the cell below\n",
    "torch.cuda.set_device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b73dc6cc-e26d-4c87-bb02-aa0bcc511027",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version? 1.13.0+cu117\n",
      "Torchvision version? 0.14.0+cu117\n",
      "Is cuda available? True\n",
      "Is cuDNN version: 8500\n",
      "cuDNN enabled?  True\n",
      "Device count? 1\n",
      "Current device? 0\n",
      "Device name?  NVIDIA GeForce RTX 3060\n",
      "tensor([[0.8968, 0.3288, 0.3419],\n",
      "        [0.7293, 0.7448, 0.0672],\n",
      "        [0.2199, 0.6071, 0.0307],\n",
      "        [0.6126, 0.1665, 0.2358],\n",
      "        [0.4057, 0.3819, 0.6767]])\n"
     ]
    }
   ],
   "source": [
    "# Check that the selected device is the desired one\n",
    "print(\"Torch version?\", torch.__version__)\n",
    "print(\"Torchvision version?\", torchvision.__version__)\n",
    "print(\"Is cuda available?\", torch.cuda.is_available())\n",
    "print(\"Is cuDNN version:\", torch.backends.cudnn.version())\n",
    "print(\"cuDNN enabled? \", torch.backends.cudnn.enabled)\n",
    "print(\"Device count?\", torch.cuda.device_count())\n",
    "print(\"Current device?\", torch.cuda.current_device())\n",
    "print(\"Device name? \", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fedb5c2-607a-4b96-8bfb-b6dbd126bfa5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Config and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa409b27-24ff-464e-869f-06166c57ed36",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.001\n",
    "        self.num_epochs = 100\n",
    "        self.batch_size = 64\n",
    "        self.patience = 10\n",
    "        self.dropout_p = 0.3\n",
    "        self.embedding_size = 128\n",
    "        self.scheduler_step_size = 70\n",
    "        self.scheduler_gamma = 0.1\n",
    "        # Other application variables\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.base_path = \"./output\"\n",
    "        os.makedirs(self.base_path, exist_ok=True)  # Create the base_path directory if it doesn't exist\n",
    "        self.best_model_path = os.path.join(self.base_path, \"best_model.pth\")\n",
    "        self.last_model_path = os.path.join(self.base_path, \"last_model.pth\")\n",
    "        self.learning_plot_path = os.path.join(self.base_path, \"learning_curves.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f836428-ecd0-46de-a98a-95bfd9b0b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, _ = self.data[idx]  # We don't need labels for SimCLR\n",
    "\n",
    "        if self.transform:\n",
    "            augmented_image_1 = self.transform(image)\n",
    "            augmented_image_2 = self.transform(image)\n",
    "\n",
    "        return augmented_image_1, augmented_image_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a433235-8a9a-430e-8b3e-55de36b5a9fe",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4af8d2ed-0813-4b4d-957e-e1b7a34fbb8f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, dropout_p=0.5, embedding_size=128):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.backbone = models.resnet18(weights='ResNet18_Weights.DEFAULT')\n",
    "        #self.backbone = models.resnet50(weights='ResNet50_Weights.DEFAULT')\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.backbone.fc.in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(512, embedding_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.backbone(x1)\n",
    "        x1 = self.head(x1)\n",
    "        x2 = self.backbone(x2)\n",
    "        x2 = self.head(x2)\n",
    "        \n",
    "        return x1, x2\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, z1, z2):\n",
    "        # Normalize the representations along the batch dimension\n",
    "        z1_norm = (z1 / z1.norm(dim=1)[:, None])\n",
    "        z2_norm = (z2 / z2.norm(dim=1)[:, None])\n",
    "\n",
    "        # Compute the cosine similarity matrix \n",
    "        # We add the temperature as a scaling factor (usually set to 0.5 or 0.1)\n",
    "        representations = torch.cat([z1_norm, z2_norm], dim=0)\n",
    "        similarity_matrix = torch.mm(representations, representations.t()) / self.temperature\n",
    "\n",
    "        # Compute the loss\n",
    "        batch_size = z1_norm.shape[0]\n",
    "        contrastive_loss = torch.nn.functional.cross_entropy(\n",
    "            similarity_matrix, torch.arange(2*batch_size).to(device)\n",
    "        )\n",
    "        \n",
    "        return contrastive_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18ea41c-2cb5-4933-939e-9a8364236355",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "964f1097-86f0-423c-8b91-fe3ebb0f9cfd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict(model, img1, img2, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    img1, img2 = img1.to(device), img2.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output1, output2 = model(img1, img2)\n",
    "        \n",
    "        # Normalize the output vectors\n",
    "        output1 = F.normalize(output1, p=2, dim=1)\n",
    "        output2 = F.normalize(output2, p=2, dim=1)\n",
    "        \n",
    "        # Compute the cosine similarities\n",
    "        similarities = (output1 * output2).sum(dim=1).cpu().numpy()\n",
    "        \n",
    "    return similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84ba7eb7-4b43-47a5-883a-d42e10a1c8dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot training function\n",
    "def plot_training(train_loss_history, val_loss_history, config):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_loss_history, label='Train Loss')\n",
    "    plt.plot(val_loss_history, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(config.learning_plot_path)\n",
    "    plt.show()\n",
    "\n",
    "# Plot prediction function\n",
    "def plot_prediction(img1, img2, similarities, limit=None):\n",
    "    if limit is not None:\n",
    "        img1, img2, similarities = img1[:limit], img2[:limit], distances[:limit]\n",
    "\n",
    "    fig, axs = plt.subplots(len(img1), 2, figsize=(5, 3*len(img1)))\n",
    "    for i in range(len(img1)):\n",
    "        img1_i = img1[i].permute(1, 2, 0) if img1[i].shape[0] == 3 else img1[i].squeeze()\n",
    "        img2_i = img2[i].permute(1, 2, 0) if img2[i].shape[0] == 3 else img2[i].squeeze()\n",
    "\n",
    "        cmap1 = None if img1[i].shape[0] == 3 else 'gray'\n",
    "        cmap2 = None if img2[i].shape[0] == 3 else 'gray'\n",
    "\n",
    "        axs[i, 0].imshow(img1_i.cpu(), cmap=cmap1)\n",
    "        axs[i, 1].imshow(img2_i.cpu(), cmap=cmap2)\n",
    "        axs[i, 0].axis('off')\n",
    "        axs[i, 1].axis('off')\n",
    "        axs[i, 1].set_title(f\"Similarity: {similarities[i]:.2f}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fafaac6-54e8-4613-b030-f06ab7625237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model function\n",
    "def save_model(model, save_path):\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "# Load model function\n",
    "def load_model(model, load_path, device):\n",
    "    model.load_state_dict(torch.load(load_path, map_location=device))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c1e18c-5fcb-4204-91ae-8e25d1fe2622",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1186db18-ade8-4dad-8bc6-45c5024d5ba7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, train_loader, val_loader, criterion, optimizer, scheduler, config, output_freq=2):\n",
    "    model.train()\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    best_val_loss = float('inf')\n",
    "    no_improve_epochs = 0\n",
    "    total_batches = len(train_loader)\n",
    "    print_every = total_batches // output_freq  # Print every 1/output_freq of total batches\n",
    "\n",
    "    for epoch in range(config.num_epochs):\n",
    "        start_time = time.time()\n",
    "        train_loss = 0\n",
    "        model.train()\n",
    "        for i, (img1, img2) in enumerate(train_loader):\n",
    "            img1, img2 = img1.to(config.device), img2.to(config.device)\n",
    "            output1, output2 = model(img1, img2)\n",
    "            loss = criterion(output1, output2)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Print training loss\n",
    "            if i % print_every == 0:\n",
    "                print(f\"Epoch: {epoch+1}, Batch: {i+1}, Loss: {loss.item()}\")\n",
    "\n",
    "        scheduler.step()\n",
    "        train_loss_history.append(train_loss / len(train_loader))\n",
    "\n",
    "        #val_loss = validate(model, val_loader, criterion, config)\n",
    "        val_loss = train_loss / len(train_loader)\n",
    "        val_loss_history.append(val_loss)\n",
    "        end_time = time.time()\n",
    "        epoch_time = end_time - start_time\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}, Loss: {train_loss_history[-1]}, Val Loss: {val_loss}, Time: {epoch_time}s, Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
    "\n",
    "        # Save last model\n",
    "        save_model(model, config.last_model_path)    \n",
    "\n",
    "        # Save best model & early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_model(model, config.best_model_path)\n",
    "            no_improve_epochs = 0\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            if no_improve_epochs >= config.patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    return train_loss_history, val_loss_history\n",
    "\n",
    "\n",
    "# Validation function\n",
    "def validate(model, val_loader, criterion, config):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (img1, img2) in enumerate(val_loader):\n",
    "            img1, img2 = img1.to(config.device), img2.to(config.device), labels.to(config.device)\n",
    "            output1, output2 = model(img1, img2)\n",
    "            loss = criterion(output1, output2)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6554d9-d911-45bc-8eac-b2d97226a675",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "625e794c-4c5e-48dd-b51f-9cae4c66143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate(model, test_loader, config, limit=None):\n",
    "    model = model.to('cpu')\n",
    "    model.eval()\n",
    "    positive_distances = []\n",
    "    negative_distances = []\n",
    "    labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        count = 0\n",
    "        for img1, img2, labels in tqdm(test_loader, desc=\"Evaluating\", unit=\"batch\"):\n",
    "            count += 1\n",
    "            output1, output2 = model(img1, img2)\n",
    "            distances = F.pairwise_distance(output1, output2).detach().numpy()\n",
    "            labels = labels.numpy()\n",
    "\n",
    "            positive_distances.extend(distances[labels == 1])\n",
    "            negative_distances.extend(distances[labels == 0])\n",
    "            labels_list.extend(labels)\n",
    "\n",
    "            if limit is not None:\n",
    "                if count > limit:\n",
    "                    break\n",
    "\n",
    "    # Compute best threshold\n",
    "    distances = positive_distances + negative_distances\n",
    "    labels = np.array([1]*len(positive_distances) + [0]*len(negative_distances))\n",
    "    fpr, tpr, thresholds = roc_curve(labels, distances, pos_label=0)\n",
    "    best_threshold = thresholds[np.argmax(tpr - fpr)]\n",
    "\n",
    "    # Compute histograms\n",
    "    plt.hist(positive_distances, bins=30, alpha=0.5, color='r', label='Positive pairs')\n",
    "    plt.hist(negative_distances, bins=30, alpha=0.5, color='b', label='Negative pairs')\n",
    "    \n",
    "    # Plot best threshold\n",
    "    plt.axvline(x=best_threshold, color='g', linestyle='--', label=f'Best threshold: {best_threshold:.2f}')\n",
    "    plt.legend()\n",
    "    plt.savefig(config.threshold_plot_path)\n",
    "    plt.show()\n",
    "\n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06443d7-5d82-47ac-801d-40d0de5a6d5c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Main Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "261571d0-b2bd-4ca1-9b9e-4ab1ba02f73a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Main function\n",
    "def main(do_train=False):\n",
    "    config = Config()\n",
    "\n",
    "    # CIFAR10 Mean and Std Dev for normalization\n",
    "    cifar10_mean = [0.4914, 0.4822, 0.4465]\n",
    "    cifar10_std = [0.2023, 0.1994, 0.2010]\n",
    "    \n",
    "    # Define transformations\n",
    "    data_transforms = Compose([\n",
    "        RandomResizedCrop(32),\n",
    "        RandomHorizontalFlip(),\n",
    "        RandomApply([ColorJitter(0.8, 0.8, 0.8, 0.2)], p=0.8),\n",
    "        RandomGrayscale(p=0.2),\n",
    "        RandomApply([GaussianBlur([.1, 2.])], p=0.5),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=cifar10_mean, std=cifar10_std)\n",
    "    ])\n",
    "    \n",
    "    # Initialize datasets\n",
    "    trainset = ImageDataset(CIFAR10(root='./data', train=True, download=True), transform=data_transforms)\n",
    "    testset = ImageDataset(CIFAR10(root='./data', train=False, download=True), transform=data_transforms)\n",
    "    \n",
    "    # Prepare DataLoader\n",
    "    train_loader = DataLoader(trainset, batch_size=config.batch_size, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(testset, batch_size=config.batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "    # Instantiate the model, criterion, optimizer, and scheduler\n",
    "    # We need to have a model also for the case in whic we don't train\n",
    "    model = SiameseNetwork(embedding_size=config.embedding_size,\n",
    "                           dropout_p=config.dropout_p).to(config.device)\n",
    "    \n",
    "    criterion = ContrastiveLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
    "    scheduler = StepLR(optimizer, step_size=config.scheduler_step_size, gamma=config.scheduler_gamma)\n",
    "\n",
    "    # TRAIN!\n",
    "    if do_train:\n",
    "        train_loss_history, val_loss_history = train(model, train_loader, val_loader, criterion, optimizer, scheduler, config, debug=True)\n",
    "\n",
    "    # Load the best model\n",
    "    model = SiameseNetwork(embedding_size=config.embedding_size,\n",
    "                           dropout_p=config.dropout_p).to(config.device)\n",
    "    model = load_model(model, config.best_model_path, config.device)\n",
    "\n",
    "    # Evaluate\n",
    "    print(\"Evaluating model...\")\n",
    "    print(\"Evaluation completed!\")\n",
    "    \n",
    "    # Test the model\n",
    "    print(\"Predicting random batch...\")\n",
    "    test_img1, test_img2, _ = next(iter(test_loader))\n",
    "    similarities = predict(model, test_img1, test_img2)\n",
    "\n",
    "    # Plot predictions\n",
    "    plot_prediction(test_img1.to(\"cpu\"), test_img2.to(\"cpu\"), distances, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d13aca9-a247-4be6-ad49-fc7b342ed265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
